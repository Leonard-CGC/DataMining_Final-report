{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45c02fd-9078-4354-85f8-36ab824e1603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7e4d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix, normalized_mutual_info_score\n",
    "from unidip.dip import diptst\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fed539-e3f6-4ef3-8d1c-fe60bc3a1e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Will use {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bf5120",
   "metadata": {},
   "source": [
    "#### Define Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33057043-984e-4262-9b15-bad788e81150",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d, m=5):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(nn.Linear(d, 500),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(500, 500),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(500, 2000),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(2000, m),)\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.encoder(inputs)\n",
    "        return outputs\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d, m=5):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(nn.Linear(m, 2000),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(2000, 500),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(500, 500),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(500, d),\n",
    "                                     nn.Sigmoid())\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.encoder(inputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d9d099",
   "metadata": {},
   "source": [
    "#### Define Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f712d901-3fc2-40c2-8f49-b34f13ef8375",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lrec(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Lrec, self).__init__()\n",
    "        \n",
    "    def forward(self, output, targets):\n",
    "        return torch.mean(torch.sum(torch.square(output-targets), dim=-1))\n",
    "    \n",
    "class Lclu(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Lclu, self).__init__()\n",
    "        \n",
    "    def forward(self, encoded_inputs, encoded_centres, Dc, P):\n",
    "        k = Dc.shape[0]\n",
    "        mean = torch.sum(Dc)/(k**2-k)\n",
    "        std = torch.sqrt(torch.sum(torch.square(Dc-mean))/(k**2-k))\n",
    "        return ((1+std)/mean)*torch.mean(torch.sum(P*torch.sum(torch.square(torch.unsqueeze(encoded_inputs, 1)-encoded_centres),-1),-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4165e60",
   "metadata": {},
   "source": [
    "#### Define pre-train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35219410-a39e-4cd3-baf4-a58eb000af05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pre_train_AE(dataset, epochs=100, lr=0.001, batch_size=256):\n",
    "    N_feature = np.prod(dataset.data.shape[1:])\n",
    "    data = dataset.data.view(-1, N_feature).to(device)\n",
    "    enc = Encoder(N_feature).to(device)\n",
    "    dec = Decoder(N_feature).to(device)\n",
    "    optimizer1 = torch.optim.Adam(enc.parameters(), lr=lr)\n",
    "    optimizer2 = torch.optim.Adam(dec.parameters(), lr=lr)\n",
    "    Lrec_function = Lrec().to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        random_perm = np.random.permutation(len(dataset))\n",
    "        mini_batch_index = 0\n",
    "        while True:\n",
    "            indices = random_perm[mini_batch_index:mini_batch_index + batch_size]\n",
    "            inputs = data[indices]\n",
    "            \n",
    "            enc.zero_grad() #清空前一次的gradient\n",
    "            dec.zero_grad() #清空前一次的gradient\n",
    "            \n",
    "            encoded = enc(inputs)\n",
    "            decoded = dec(encoded)\n",
    "            loss = Lrec_function(decoded, inputs)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer1.step()\n",
    "            optimizer2.step()\n",
    "            total_loss += loss\n",
    "            \n",
    "            mini_batch_index += batch_size\n",
    "            if mini_batch_index >= len(dataset):\n",
    "                break\n",
    "        total_loss /= len(dataset)\n",
    "        \n",
    "        print(f'[{epoch+1}/{epochs}] Loss: {total_loss}')\n",
    "    return enc, dec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33ab4f0",
   "metadata": {},
   "source": [
    "#### Define DipDECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3ae1f8-1bb5-4930-b182-eb1fa041506d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DipDECK:\n",
    "    def __init__(self, dataset, enc, dec, k_init=15, P_threshold=0.9, epochs=50, batch_size = 256):\n",
    "        self.BATCHSIZE = batch_size\n",
    "        self.N_DATA, self.N_FEATURE = len(dataset), np.prod(dataset.data.shape[1:])\n",
    "        self.data = dataset.data.type(torch.float32).view(-1,self.N_FEATURE)\n",
    "        self.P_threshold = P_threshold\n",
    "        self.k = k_init\n",
    "        self.enc = enc\n",
    "        self.dec = dec\n",
    "        print('******************** Apply K-means ********************')\n",
    "        with torch.no_grad():\n",
    "            Km_model = KMeans(self.k)\n",
    "            Km_model.fit(enc(self.data.to(device)).cpu())\n",
    "        self.kmCentres = torch.tensor(Km_model.cluster_centers_, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(Km_model.labels_, dtype=torch.int64)\n",
    "        print('********** finding closest points to kmCentres **********')\n",
    "        self.centres = self.find_centres1()\n",
    "        # print('******************** building DipMatrix ********************')\n",
    "        self.DipMatrix = self.build_DipMatrix()\n",
    "        print('******************** Batch optimizing ********************')\n",
    "        self.Batch_optimize(epochs)\n",
    "        \n",
    "    def Batch_optimize(self, epochs, lr=0.0001):\n",
    "        optimizer1 = torch.optim.Adam(self.enc.parameters(), lr=lr)\n",
    "        optimizer2 = torch.optim.Adam(self.dec.parameters(), lr=lr)\n",
    "        Lrec_function = Lrec().to(device)\n",
    "        Lclu_function = Lclu().to(device)\n",
    "        \n",
    "        i = 0\n",
    "        while i<epochs:\n",
    "            print(f'********** epoch {i+1} of {epochs} **********')\n",
    "            random_perm = np.random.permutation(self.N_DATA)\n",
    "            mini_batch_index = 0\n",
    "            j = 0\n",
    "            while True:\n",
    "                indices = random_perm[mini_batch_index:mini_batch_index + self.BATCHSIZE]\n",
    "                if i!=0:\n",
    "                    self.update_labels(indices)\n",
    "                inputs = self.data[indices].to(device)\n",
    "                encoded_centres = self.enc(self.centres.to(device))\n",
    "                r = torch.sum(torch.square(encoded_centres), axis=1)\n",
    "                Dc = torch.sqrt((r+encoded_centres@encoded_centres.T).T+r)\n",
    "                specific_PMetrix = self.nDipMatrix[torch.unsqueeze(self.labels[indices],0)[0]].to(device)\n",
    "                \n",
    "                self.enc.zero_grad()\n",
    "                self.dec.zero_grad()\n",
    "                \n",
    "                encoded = self.enc(inputs)\n",
    "                decoded = self.dec(encoded)\n",
    "                L1 = Lrec_function(decoded, inputs)\n",
    "                L2 = Lclu_function(encoded, encoded_centres, Dc, specific_PMetrix)\n",
    "                L = L1 + L2\n",
    "                L.backward()\n",
    "                \n",
    "                optimizer1.step()\n",
    "                optimizer2.step()\n",
    "                print(f'[{j+1}/{self.N_DATA//self.BATCHSIZE+1 if self.N_DATA%self.BATCHSIZE else self.N_DATA//self.BATCHSIZE}] batch loss:{L/self.BATCHSIZE} (Lrec:{L1/self.BATCHSIZE}, Lclu:{L2/self.BATCHSIZE})')\n",
    "                \n",
    "                mini_batch_index += self.BATCHSIZE\n",
    "                if mini_batch_index >= self.N_DATA:\n",
    "                    break\n",
    "                j += 1\n",
    "                    \n",
    "            self.update_labels(range(self.k))\n",
    "            self.centres = self.find_centres2()\n",
    "            self.DipMatrix = self.build_DipMatrix()\n",
    "            i += 1\n",
    "            with torch.no_grad():\n",
    "                while torch.max(self.DipMatrix-torch.eye(self.k))>=self.P_threshold:\n",
    "                    self.k -= 1\n",
    "                    print(f'********** merging (remain {self.k} cluster) **********')\n",
    "                    argmax = torch.argmax(self.DipMatrix-torch.eye(self.k+1))\n",
    "                    Ci, Cj = argmax//self.DipMatrix.shape[1], argmax%self.DipMatrix.shape[1]\n",
    "                    Ci, Cj = (Ci, Cj) if Ci<=Cj else (Cj, Ci)\n",
    "                    self.labels[self.labels==Cj] = Ci\n",
    "                    self.labels[self.labels>Cj] = self.labels[self.labels>Cj]-1\n",
    "                    new_centre = self.find_merged_centre(Ci, Cj)\n",
    "                    self.centres[Ci] = new_centre\n",
    "                    self.centres = torch.concat([self.centres[:Cj,:], self.centres[Cj+1:,:]], 0)\n",
    "                    self.DipMatrix = self.update_DipMatrix(Ci, Cj)\n",
    "                    i = 0\n",
    "            \n",
    "    @torch.no_grad()\n",
    "    def find_centres1(self):\n",
    "        encoded_data = self.enc(self.data.to(device))\n",
    "        dist = torch.sum(torch.square(encoded_data-torch.unsqueeze(self.kmCentres.to(device), 1)),-1)\n",
    "        return self.data[torch.argmin(dist, -1)]\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def find_centres2(self):\n",
    "        encoded_data = self.enc(self.data.to(device))\n",
    "        centres = torch.zeros(self.k, self.data.shape[1]).to(device)\n",
    "        for i in range(self.k):\n",
    "            centre_ = torch.mean(encoded_data[self.labels==i], axis=0)\n",
    "            dist = torch.sum(torch.square(encoded_data-centre_), axis=-1)\n",
    "            centres[i] = self.data[torch.argmin(dist)]\n",
    "        return centres\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def find_merged_centre(self, Ci, Cj):\n",
    "        data_CiCj = self.data[torch.logical_or(self.labels==Ci,self.labels==Cj)].to(device)\n",
    "        encoded_data_CiCj = self.enc(data_CiCj)\n",
    "        encoded_centres = self.enc(self.centres.to(device))\n",
    "        centre_Ci, centre_Cj = encoded_centres[Ci], encoded_centres[Cj]\n",
    "        N_Ci, N_Cj = torch.sum(self.labels==Ci), torch.sum(self.labels==Cj)\n",
    "        weighted_centre = (N_Ci*centre_Ci+N_Cj*centre_Cj)/(N_Ci+N_Cj)\n",
    "        dist = torch.sum(torch.square(encoded_data_CiCj-weighted_centre), axis=-1)\n",
    "        return data_CiCj[torch.argmin(dist)].cpu()\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def build_DipMatrix(self):\n",
    "        print(f'******************** building DipMatrix ********************')\n",
    "        encoded_data = self.enc(self.data.to(device)).cpu()\n",
    "        encoded_centres = self.enc(self.centres.to(device)).cpu()\n",
    "        dip_matrix = torch.eye(self.k, dtype=torch.float32)\n",
    "        with tqdm(total=self.k*(self.k-1)//2) as pbar:\n",
    "            for i in range(self.k):\n",
    "                for j in range(i+1,self.k):\n",
    "                    points = encoded_data[torch.logical_or(self.labels==i,self.labels==j)]\n",
    "                    C_1d = torch.sum(points*(encoded_centres[i]-encoded_centres[j]), dim=-1)\n",
    "#                     C_1d_hist, _ = torch.histogram(C_1d, bins=int(len(C_1d)/3))\n",
    "                    P1 = diptst(C_1d.numpy(), is_hist=False)[1]\n",
    "                    N_Ci, N_Cj = torch.sum(self.labels==i), torch.sum(self.labels==j)\n",
    "                    (Ci, Cj, N_Ci, N_Cj) = (i, j, N_Ci, N_Cj) if N_Ci<=N_Cj else (j, i, N_Cj, N_Ci)\n",
    "                    if N_Cj>2*N_Ci and N_Ci!=0:\n",
    "                        points_Cj = encoded_data[self.labels==Cj]\n",
    "                        dist2Ci = torch.sum(torch.square(points_Cj-encoded_centres[Ci]), dim=-1)\n",
    "                        partition_points_Cj = points_Cj[dist2Ci<=np.max(np.partition(dist2Ci,2*N_Ci)[:2*N_Ci])]\n",
    "                        points = torch.concat([encoded_data[self.labels==Ci], partition_points_Cj])\n",
    "                        C_1d = torch.sum(points*(encoded_centres[i]-encoded_centres[j]), axis=-1)\n",
    "#                         C_1d_hist, _ = torch.histogram(C_1d, bins=int(len(C_1d)/3))\n",
    "                        P2 = diptst(C_1d.numpy(), is_hist=False)[1]\n",
    "                        dip_matrix[i,j] = dip_matrix[j,i] = min(P1, P2)\n",
    "                    else:\n",
    "                        dip_matrix[i,j] = dip_matrix[j,i] = P1\n",
    "                    pbar.update(1)\n",
    "        self.nDipMatrix = self.Matrix2affine(dip_matrix)\n",
    "        return dip_matrix\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def update_DipMatrix(self, idx, jdx):\n",
    "        print(f'******************** updating DipMatrix ********************')\n",
    "        encoded_data = self.enc(self.data.to(device)).cpu()\n",
    "        encoded_centres = self.enc(self.centres.to(device)).cpu()\n",
    "        dip_matrix = torch.concat([self.DipMatrix[:,:jdx], self.DipMatrix[:,jdx+1:]], 1)\n",
    "        dip_matrix = torch.concat([dip_matrix[:jdx,:], dip_matrix[jdx+1:,:]], 0)\n",
    "        with tqdm(total=self.k-1) as pbar:\n",
    "            for j in range(self.k):\n",
    "                if j==idx:\n",
    "                    continue\n",
    "                points = encoded_data[torch.logical_or(self.labels==idx,self.labels==j)]\n",
    "                C_1d = torch.sum(points*(encoded_centres[idx]-encoded_centres[j]), axis=-1)\n",
    "#                 C_1d_hist, _ = torch.histogram(C_1d, bins=int(len(C_1d)/3))\n",
    "                P1 = diptst(C_1d.numpy(), is_hist=False)[1]\n",
    "                N_Ci, N_Cj = torch.sum(self.labels==idx), torch.sum(self.labels==j)\n",
    "                (Ci, Cj, N_Ci, N_Cj) = (idx, j, N_Ci, N_Cj) if N_Ci<=N_Cj else (j, idx, N_Cj, N_Ci)\n",
    "                if N_Cj>2*N_Ci and N_Ci!=0:\n",
    "                    points_Cj = encoded_data[self.labels==Cj]\n",
    "                    dist2Ci = torch.sum(torch.square(points_Cj-encoded_centres[Ci]), axis=-1)\n",
    "                    partition_points_Cj = points_Cj[dist2Ci<=np.max(np.partition(dist2Ci,2*N_Ci)[:2*N_Ci])]\n",
    "                    points = torch.concat([encoded_data[self.labels==Ci], partition_points_Cj])\n",
    "                    C_1d = torch.sum(points*(encoded_centres[idx]-encoded_centres[j]), axis=-1)\n",
    "#                     C_1d_hist, _ = torch.histogram(C_1d, bins=int(len(C_1d)/3))\n",
    "                    P2 = diptst(C_1d.numpy(), is_hist=False)[1]\n",
    "                    dip_matrix[idx,j] = dip_matrix[j,idx] = min(P1, P2)\n",
    "                else:\n",
    "                    dip_matrix[idx,j] = dip_matrix[j,idx] = P1\n",
    "                pbar.update(1)\n",
    "        self.nDipMatrix = self.Matrix2affine(dip_matrix)\n",
    "        print(dip_matrix)\n",
    "        return dip_matrix\n",
    "    \n",
    "    def Matrix2affine(self, matrix):\n",
    "        return (matrix/matrix.sum(1)).T\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def update_labels(self, indices):\n",
    "        encoded_data = self.enc(self.data.to(device))\n",
    "        encoded_centres = self.enc(self.centres.to(device))\n",
    "        D = torch.sum(torch.square(encoded_data-torch.unsqueeze(encoded_centres, 1)),-1)\n",
    "        new_labels = torch.argmin(D, axis=0).type(torch.int64).cpu()\n",
    "        self.labels[indices] = new_labels[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bea190-1a5e-4c64-9390-9ecca8c891db",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2e2d7c-ec4e-44f6-a56b-604cb76433cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54a8575-3349-41bb-bc57-01e100340a53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST('./dataset',\n",
    "                            train=True,\n",
    "                            download=False,\n",
    "                            transform=transforms.ToTensor())\n",
    "train_data.data = train_data.data/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be52aff8-b670-426f-9846-edae4ba2cc48",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "enc, dec = Pre_train_AE(train_data, epochs=100, lr=0.001)\n",
    "end = time.time()\n",
    "print(f'\\n預訓練耗時{str(datetime.timedelta(seconds=end-start))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4163eaae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "Model = DipDECK(train_data, enc.to(device), dec.to(device), k_init=35, epochs=50)\n",
    "end = time.time()\n",
    "print(f'共分了{Model.k}群，耗時{str(datetime.timedelta(seconds=end-start))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b6cc62",
   "metadata": {},
   "source": [
    "#### confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed181425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Data=60000, #pre-train epoch=100, #train epoch=50\n",
    "fig = plt.Figure(figsize=(5,5))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.set_xlabel('cluster', fontsize=16)\n",
    "ax.set_ylabel('real label', fontsize=16)\n",
    "confusionmatrix = confusion_matrix(train_data.targets, Model.labels)\n",
    "ax.set_xticks(range(confusionmatrix.shape[1]))\n",
    "ax.set_yticks(range(confusionmatrix.shape[0]))\n",
    "ax.imshow(confusionmatrix[:,:-1], cmap='PuRd')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d6f4da",
   "metadata": {},
   "source": [
    "#### rearranged confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34edb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_confusion_matrix = confusionmatrix[:,[2,3,0,1,4,5,6,8,7]]\n",
    "\n",
    "fig = plt.Figure(figsize=(5,5))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.set_xlabel('rearranged cluster', fontsize=16)\n",
    "ax.set_ylabel('real', fontsize=16)\n",
    "ax.set_xticks(range(confusionmatrix.shape[1]))\n",
    "ax.set_yticks(range(confusionmatrix.shape[0]))\n",
    "ax.imshow(sorted_confusion_matrix, cmap='PuRd')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdce62e",
   "metadata": {},
   "source": [
    "#### NMI score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862c7c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'NMI={normalized_mutual_info_score(train_data.targets, Model.labels)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ca1f50",
   "metadata": {},
   "source": [
    "#### Show the Origian, encoded and decoded train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b71c89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "\n",
    "label = 10\n",
    "\n",
    "encoder = Model.enc.cpu()\n",
    "decoder = Model.dec.cpu()\n",
    "\n",
    "dat = train_data.data.view(-1,784)[Model.labels==label]\n",
    "encoded_img = encoder(dat)\n",
    "decoded_img = decoder(encoded_img)\n",
    "\n",
    "encoded_img = encoded_img.detach().numpy()\n",
    "decoded_img = decoded_img.detach().numpy()\n",
    "\n",
    "def f(i):\n",
    "    fig = plt.Figure(figsize=(10,10))\n",
    "    ax1 = fig.add_subplot(1,3,1)\n",
    "    ax2 = fig.add_subplot(1,3,2)\n",
    "    ax3 = fig.add_subplot(1,3,3)\n",
    "    ax1.axis('off')\n",
    "    # ax2.axis('off')\n",
    "    ax3.axis('off')\n",
    "    ax1.imshow(dat[i].reshape(28,28), cmap='gray')\n",
    "    ax2.imshow(encoded_img[i].reshape(1,5), cmap='gray')\n",
    "    print(encoded_img[i].reshape(1,5))\n",
    "    ax3.imshow(decoded_img[i].reshape(28,28), cmap='gray')\n",
    "    return fig\n",
    "    \n",
    "interact(f, i=widgets.IntSlider(min=0, max=dat.shape[0]-1, step=1, value=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ed88d3",
   "metadata": {},
   "source": [
    "#### save DipDECK model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec0eab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename= # your file name\n",
    "\n",
    "with open(f'{filename}.pkl', 'wb') as outp:\n",
    "    pickle.dump((enc, dec), outp, pickle.HIGHEST_PROTOCOL)\n",
    "    pickle.dump(Model, outp, pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
